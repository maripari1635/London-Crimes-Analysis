---
title: "Crimes of London: Perspective Analysis on Crimes within England's Capital"
author: "Mackenzie Ripari"
date: "7/6/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$$\\[1in]$$

# Abstract
Crime occurs throughout the world. As such the ability to predict the occurrence or instances of crimes would redoubtably be of worth to discover. Using crimes in the city of London dataset, aptly called london_crimes, this report aims to predict crimes with an accuracy of 70%. The creation of a model to predict the occurrence of crime is ultimately developed after exploration and minimal algorithm production of a recommendation model that does not quite work well enough. Effects within the dataset are also explored, such as the occurrence of crime and the prevalence of location effecting not just crime report rates but their being effected on by other factors. Once done a look at various other models is done, with a final model being developed as a result of the best predicting model looked at. With said final model final predictions are made and the goal of a minimal 70% accurate model is completed. Afterward a discussion on improvements and afterthoughts is done along with a final closing on this report.



\newpage



# 1.0 Introduction

## 1.1 Preliminary Outline
Crime exists. Defined by Cornell Law as "any act or omission in violation of a law prohibiting the action or omission" crime
has an impact on society at large (Cornell, 2021). It would stand to reason then that crime as a whole is documented and analyzed in order to reduce rates or see why it occurs. Indeed, there are a multitude of resources detailing the various counts and records of crime around the world. One such record details the crime within London from the years 2008 to 2016. This record, aptly named London Crimes, holds a plethora of information that can be used to analyze crime not only to see incidence but also to see where crime occurs. This can then be used to predict crimes.

This report will showcase a multitude of methods in the prediction of crime rates. Primarily, this will be shown using a method unsuited towards this type of prediction and a set of methods suited towards the prediction. As a method of motivation, this report will have a goal of creating a method that predicts with at minimum 70% accuracy crime occurrences within a given month. To begin there will be an introduction to the dataset itself; background on column meaning and in general what the dataset entails. From there there will be a breakdown on various exploratory and analysis done to see trends and importance within the dataset. Moving on from there a breakdown on the three methods earlier will be shown. From there the best method will be utilized with a portioned off set of the London Crimes dataset to predict crime occurrences. To end there will be a recap of the report as a whole, summarizing key points within the data, before ending with final thoughts and improvement opportunities. 

Before moving onward to the description portion of this report the required libraries, repositories of code and functionality, and memory size increase to handle code throughout this report need to be loaded.

```{r, warning = FALSE, message=FALSE, echo = FALSE}
#install and load libraries
if (!require(tidyverse)) install.packages('tidyverse')
library(tidyverse)
if (!require(readr)) install.packages('readr')
library(readr)
if (!require(lattice)) install.packages('lattice')
library(lattice)
if (!require(caret)) install.packages('caret')
library(caret)
if (!require(scales)) install.packages('scales')
library(scales)
if (!require(randomForest)) install.packages('randomForest')
library(randomForest)
if (!require(rpart)) install.packages('rpart')
library(rpart)
if (!require(parallel)) install.packages('parallel')
library(parallel)
if (!require(knitr)) install.packages('knitr')
library(knitr)
```

```{r, warning = FALSE}

##INCREASES MEMORY LIMIT WITHIN R
memory.limit(size=100000) 

```


## 1.2 London Crimes: Dataset Description
Before this report documents its exploration of the London Crime dataset some explanation of pertinent background is required. To begin this London Crimes dataset, this record obtained via Kaggle, holds information on the monthly incidence of crimes within London and its surrounding boroughs (Boysen, 2017). For a breakdown of columns within this dataset before wrangling see below.

1. lsoa_code: code for Lower Super Output Area in Greater London. For those unfamiliar with this terminology think of it as akin to cenus data denoting smaller population areas. 
2. borough: Common name for London borough. Akin in many ways to a city/district within a city.
3. major_category: High level categorization of crime. For instance Murder and Theft would be two
4. minor_category: Low level categorization of crime within major category. Examples include stealing vehicles or attempted assault.
5. value: monthly reported count of categorical crime in given borough. Of note here is that there may in fact be no reported crime within a given month, giving a value of 0.
6. year: Year that the value was given ranging from 2008 to 2016.
7. month: Month that the value was given in numeric form, meaning from 1 to 12.

As noted above this is before wrangling occurs. Due to hardware limitations this report necessitates the sampling of data and selecting of what data to analyze. Before moving onto that however one final thing to note about this report. As this report is documenting the reported counts of crimes as a monthly value it is likely that more unreported crimes occurred during the various time-frames than were reported. As mentioned earlier the goal of this report concerns itself with a method of at least 70% accuracy. While unable to accounted for undocumented data this report will strive to utilize what data it can to the best of its ability.

# 2.0 London Crimes Dataset Overview:

## 2.2 Dataset Wrangling
Mentioned above hardware limitations necessitate some wrangling that occurs within this dataset. Of note this report will concern itself with crime from the year 2010 onward. Moreover due to the incidence rates of certain values, which will be explained below, a categorical column of crime Occurrence will be added, noting if a crime occurred with either a "Yes" or "No" value. The months column will also be converted to an abbreviated name for convenience as well. Finally due to the size of the data only a 1% sample will be usable to create the model due to vector size limitations which will be discussed later. Altogether there are now 8 columns within the dataset shown below with a glimpse of how the dataset is setup.

1. lsoa_code: code for Lower Super Output Area in Greater London. For those unfamiliar with this terminology think of it as akin to census data denoting smaller population areas. 
2. borough: Common name for London borough. Akin in many ways to a city/district within a city.
3. major_category: High level categorization of crime. For instance Murder and Theft would be two
4. minor_category: Low level categorization of crime within major category. Examples include stealing vehicles or attempted assault.
5. value: monthly reported count of categorical crime in given borough. Of note here is that there may in fact be no reported crime within a given month, giving a value of 0.
6. year: Year that the value was given ranging from 2012 to 2016.
7. month: Month that the value was given in an abbreviated month format.
8. crime_occurred: Categorical column determining whether a crime occurred in the specified location at the specified time.

```{r, warning = FALSE, echo = FALSE}

#load via relative path dataset into London crimes
london_crimes <- read.csv("./london.csv")

#WRANGLING
#looking at values from 2014 onward
london_crimes <- london_crimes %>% filter(year > 2012)

#add a column to indicate if a crime occured or not
#henceforth crime_occurred
london_crimes <- london_crimes %>% mutate(crime_occurred = ifelse(london_crimes$value == 0,"No","Yes"))

#change months from numeric to month names
london_crimes <- london_crimes %>% mutate(month = month.abb[as.numeric(month)])

```

```{r, warning = FALSE, echo=FALSE}

head(london_crimes, 5)

```

With the above done this report can now partition the data into two parts, a final test set for use in determining a model with an accuracy of  70% and a training set for developing the multitude of methods. These sets, final_test and london respectively, come from the london crimes dataset and are split 10% for the final_test set and 90% for the london set along with a final check to ensure data within the final_test may occur within the larger london set. With this done this report can now move onward to the exploration of the data within it.

```{r, warning = FALSE, message=FALSE}

#sample 1% of the data for use
london_crimes <- sample_frac(london_crimes, 0.01)

#set aside 10% of data for final validation and a set for training
#set seed to ensure replicability
set.seed(2021, sample.kind="Rounding")

#split for a final test set and observation set, henceforth london
test_index <- createDataPartition(y = london_crimes$value, times = 1, p = 0.1, list = FALSE)
london <- london_crimes[-test_index,]
temp <- london_crimes[test_index,]

# Make sure following are found in both final test and london
#borough major and minor cat year month crime occur value 
final_test <- temp %>% 
  semi_join(london, by = "borough")%>%
  semi_join(london, by = "major_category")%>%
  semi_join(london, by = "lsoa_code")%>%
  semi_join(london, by = "minor_category")%>%
  semi_join(london, by = "year")%>%
  semi_join(london, by = "month")%>%
  semi_join(london, by = "value")%>%
  semi_join(london, by = "crime_occurred")


# Add rows removed from final set back into london
removed <- anti_join(temp, final_test)
london <- rbind(london, removed)

##cleanup
rm(removed, temp, test_index, london_crimes)

```

## 2.3 Exploration of London Crimes
To recap here is a glimpse at the first 5 records of the london dataset, which will be used to train and create the upcoming models in this report.

```{r, warning = FALSE}

#head of data for quick overview
head(london, 5)

```

With this in mind here is a breakdown of unqiue values within each column of the dataset to see the general nuance within the data. For the sake of space only the first 5 of each category will be shown though.

```{r, warning = FALSE}

#now check distinct values for specific cat data
# lsoa code, borough, major cat, minor cat
n_distinct(london$lsoa_code)

#districts distinct and names thereof
n_distinct(london$borough)
head(distinct(london[2]),5)

#now breakdown of major cat
n_distinct(london$major_category)
head(distinct(london[3]),5)

#minor cat breakdown
n_distinct(london$minor_category)
head(distinct(london[4]),5)

```

As shown above there is quite a bit of depth to the data. Also of note is the values column, which notes the occurrence of a crime within a month. Below is a side by side of two plots depicting the occurrence of crimes by the values per month, one including all types of occurrences and the other filtering the values from 1 to 10.

```{r, warning = FALSE, echo=FALSE, fig.width=5.5,fig.height=3.2}

#add two occurence plots side by side

#quick table of value count
london %>% group_by(value)%>%
  ggplot(aes(value))+
  geom_bar() +
  labs(title = "Crime Occurrences")+
  scale_y_continuous(name="Count", labels = comma)+
  scale_x_continuous(name="Crimes Per Month")

```

```{r, warning = FALSE, echo=FALSE, fig.width=5.5,fig.height=3.5}

#more in-depth view 
london %>% group_by(value)%>%
  filter(value > 0 && value < 10) %>%
  ggplot(aes(value))+
  geom_bar() +
  labs(title = "Crime Occurrences")+
  scale_y_continuous(name="Count", labels = comma)+
  scale_x_continuous(name = "Crimes Per Month", breaks = c(0,1,2,3,4,5,6,7,8,9,10))

```

For a more numeric view of the data below is a short table showcasing values 0 to 9. Of note here is the prominence of the values 0, 1, and 
2.

```{r, warning = FALSE, message=FALSE}

#more meaningful as summarize
london %>% group_by(value) %>%
  summarise(n=n()) %>% head(10)

```
As seen above there is a prominent skew of the data towards no crimes being reported within a month. As such it may be prudent to view the data in categorical terms of a binary yes or no response. Shown below is a short plot of such, with the incidence of whether crime occurs with a more numeric viewpoint provided as well.

```{r, warning = FALSE, echo=FALSE, fig.width=5,fig.height=5}

#now see as incidence
london %>% group_by(crime_occurred)%>%
  ggplot(aes(crime_occurred))+
  geom_bar()+  
  scale_y_continuous(name="Count", labels = comma)+
  labs(title = "Whether Crime Occurs")

london %>% group_by(crime_occurred)%>%
  summarise(n=n())

```

With this in mind here is a look at the occurence, not the amount, of crimes within each of the cities. To show the difference in this below are two plots, one counting the occurences with the value of zero being accounted for, erronously in this case, and one accouting and ignoring these values.

```{r, warning = FALSE, fig.width=4,fig.height=4}

#note, need to look at values greater than 0 when summarizing or via crime occurred
#0 here means nothing counted for specific crime in given month

#demonstrate below, comparing borough crime occurences per month
#note, just seeing if crime occured, not amount,

#incorrect plot, 
london %>%
  group_by(borough)%>%
  ggplot(aes(borough))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Occurence", labels = comma)+
  labs(title="Incorrect Plotting of Crime Occurrences")

```


```{r, warning = FALSE}
#correct plot
london %>%
  filter(crime_occurred=="Yes") %>%
  group_by(borough)%>%
  ggplot(aes(borough))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Occurence", labels = comma)+
  labs(title="Crime Occurrences Within Boroughs")

```

Notably to the above two plots are how significant the lack of a report has on the plots, with a significant decrease across the board for all borough crime occurrences. When looking at the instances of crime however the need to account for zero is already handled, and when seen below a marked incidence with the borough of Westminster with almost 3,000 crime instances.

```{r, warning = FALSE, echo=FALSE, fig.width=4,fig.height=4}

#repeat with major cat, no need demonstrate
london %>%
  filter(value > 0) %>%
  group_by(major_category)%>%
  ggplot(aes(major_category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Occurrence", labels = comma)+
  labs(title="Instances of Crime Per Type")

```

In conjuncture with the point of crime instances to the above is the marked proclivity towards two types of crime, along with their sub-types, the major_category and minor_category respectively. Shown below are plots showcasing this marked increase in crimes in Thievery and Violence Against Persons along with a breakdown of the sub-types on the next page.

```{r, warning = FALSE, echo=FALSE, fig.width=4,fig.height=4}

#here can see the amount of crime per city from 2010 to 2016, with prev comparison unneeded
#as adding zero will not change amount, only count
london %>%
  filter(value > 0) %>%
  group_by(major_category)%>%
  ggplot(aes(major_category, value))+
  geom_col()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Amount", labels = comma)+
  labs(title = "Crime Amount Per Type")

```

\newpage

```{r, warning = FALSE, echo=FALSE, fig.width=12, fig.height=16}

#minor cat, since what makes major, can be seen as stack for major
#set colors for greater readability
groupColors <- c("#696969", "#800000", "#006400", "#808000", "#483d8b", "#008b8b",
                 "#d2691e", "#9acd32", "#00008b", "#daa520", "#7f007f", "#8fbc8f",
                 "#b03060", "#ff4500", "#ffff00", "#40e0d0", "#7fff00", "#8a2be2",
                 "#00ff7f", "#dc143c", "#00bfff", "#0000ff", "#ff00ff", "#1e90ff",
                 "#fa8072", "#b0e0e6", "#90ee90", "#ff1493", "#7b68ee", "#f5deb3",
                 "#ee82ee", "#ffb6c1")

london %>%
  ggplot(aes(x = major_category, y=value))+
  geom_col(aes(fill=minor_category))+
  scale_fill_manual(values=groupColors)+
  scale_y_continuous(name="Crime Amount", labels = comma)+
  labs(title="Crime Amount Per Type by Subtypes")+
    theme(axis.text.x = element_text(angle = 90))


```


\newpage
The codes within the london dataset, the lsoa_code, are also pertinent to this analysis not just in the distribution of crime occurrences but also due to how they are related to other pieces of data within the dataset. A subset is best to describe the distributions of the former, shown below with a set of tibbles.
```{r, warning = FALSE, message=FALSE}

#better to see as summary perhaps
lsoa_Table <- london %>% group_by(lsoa_code) %>%
  summarise(n=n())

#more distinct values
top_n(as.data.frame(lsoa_Table), 3)
top_n(as.data.frame(lsoa_Table), -3)
sample_n(lsoa_Table,5)

```
Besides this the effect of the lsoa_codes in relation to other variables within the london dataset. In particular is a look at the boroughs of which these codes typically refer to. A heatmap of approximately 30 of these codes, while only a subset of the data, can be informative on their relations. Notable is Lewisham with its discontinuing lsoa_code count. 
```{r, warning = FALSE, fig.width=7,fig.height=7}

#get index of lsoa codes as char vector for filtering
set.seed(2021, sample.kind="Rounding")
lsoa_index <- as.vector(sample(london$lsoa_code,50))

#sample low amount to show readable lsoa codes
london%>%
  filter(lsoa_code %in% lsoa_index)%>%
  ggplot(aes(x=borough, y=lsoa_code))+
  geom_bin2d() +  
  scale_fill_gradient(low="blue",high="orange",trans="log10")+
  labs(title = "Boroughs by Curated Codes")+
  theme(axis.text.x = element_text(angle = 90))

```
Before moving on a final look at the two last variables in the data, the month and the year. Looking towards both the occurrence and instances there is a general level of similar results, with only small deviations in the instances in the year 2012 and the months February and July.
```{r, warning = FALSE, echo=FALSE, fig.width=3.6,fig.height=3.6}

#check year and month var
london %>%
  filter(value > 0) %>%
  group_by(year)%>%
  ggplot(aes(year))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Occurences", labels = comma)

#here can see the amount of crime per year
london %>%
  group_by(year)%>%
  ggplot(aes(year, value))+
  geom_col()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Amount", labels = comma)


london %>% mutate(month= fct_relevel(month, "Jan","Feb","Mar","Apr","May",
                                     "Jun","Jul","Aug","Sep","Oct","Nov","Dec"))%>%
  filter(value > 0) %>%
  group_by(month)%>%
  ggplot(aes(month))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Occurences", labels = comma)

#here can see the amount of crime per year
london %>% mutate(month= fct_relevel(month, "Jan","Feb","Mar","Apr","May",
                                     "Jun","Jul","Aug","Sep","Oct","Nov","Dec"))%>%
  filter(value > 0) %>%
  group_by(month)%>%
  ggplot(aes(month, value))+
  geom_col()+
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(name="Crime Amount", labels = comma)

```

## 2.4 London Crimes Recap: Thoughts and Goals
With what has previously been shown it is important to go over this report's preliminary findings. Crime occurrence values skew towards 0, inviting the prospect of a categorical viewpoint of occurrence of crimes. Lsoa_codes have a differing relation between the boroughs in the dataset, hinting at bias towards specific borough. Generally similar values in the month and year variables barring a few outlier only in the instances of reported crime, showing that there are minor effects. The methods discussed below will need to account for this. As a general step, due to the various values, there are two direct directions this report can take, one view as continuous and one as categorical. Finally as a reminder to the goal of this report is to have a minimum accuracy of 70% in predicting crime.

# 3.0 London Crimes: Recommendation Model

## 3.1 Recommendation Setup
One possible method of analyzing this dataset would be to utilize a recommendation based model due to the size and amount of data within the dataset. The idea here being to recommend the values of what may occur based on the dataset. In order to do so an RMSE, or residual mean squared error, may be used to do so as it acts very much akin to a standard deviation (Irizarry, 2021, Loss function). Notable in this dataset though is the fact that values within the dataset are whole numbers, as one cannot partially have an instance of a crime. This will then have to be taken into account when deriving the recommendation model. Before developing the model however a split in the training set, london, needs to occur for training purposes. Not only that, but to ensure accuracy data found within the test set must be found within the train set.

```{r, warning = FALSE}

#make rmse function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}


set.seed(2021, sample.kind="Rounding")
#since modeling for final test must separate the training set into two, with 10% as training test set
test_index <- createDataPartition(y = london$value, times = 1, p = 0.1, 
                                  list = FALSE)
train_set <- london[-test_index,]
test_set <- london[test_index,]

##ensure for samples data in test set has same borough, major, and code as train set sample
test_set <- test_set %>% 
  semi_join(train_set, by = "borough")%>%
  semi_join(train_set, by = "major_category")%>%
  semi_join(train_set, by = "lsoa_code")%>%
  semi_join(train_set, by = "minor_category")%>%
  semi_join(train_set, by = "year")%>%
  semi_join(train_set, by = "month")%>%
  semi_join(train_set, by = "value")%>%
  semi_join(train_set, by = "crime_occurred")

```

With this done the recommendation model can be developed in the following section.

## 3.2 Averaging the Dataset
To begin the recommendation model here is a quick base RMSE to improve from, derived via predicting all variation in the data is by chance as shown in the model below from Professor Irazarry's githhub (Irizarry, 2021, A first model). This is done via Y, the predicted value, being generated from mu, all the values in the dataset, and e, the assumed errors.
$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$
```{r, warning = FALSE}

##simple way to predict,
##using only mu, the average of random variance
set.seed(2021, sample.kind="Rounding")

mu_hat <- mean(train_set$value)
mu_hat

naive_rmse <- RMSE(test_set$value, mu_hat)

##put result into table for later use
rmse_results <- tibble(method = "Assumed Average", RMSE = naive_rmse)
rmse_results

```

As seen above this assumption is highly inaccurate and has much room for improvement. One method of improving this model is to account for the variations being from the effects of the variables. Said variables being the codes, borough, major category, minor cateogy, month, and finally year. Demonstrated below is a model of this effect. These effects can be approximated with the model below based on one shown by Professor Irizarry (Irizarry, 2021, Modeling movie effects). Notably it is similar to the model above but with the addition of v, the effects defined as average values. For the sake of brevity, please refer to the R script provided with this code for reference to code utilized

$$
Y_{u,i} = \mu + v_i + \varepsilon_{u,i}
$$
```{r, warning = FALSE, message=FALSE, echo=FALSE}

#now predict based on all effects accounted for
#these are year, month, lsao code, borough, major cat, minor cat,
set.seed(2021, sample.kind="Rounding")

mu <- mean(train_set$value) 

code_avg <- train_set %>% 
  group_by(lsoa_code) %>% 
  summarize(b_c = mean(value - mu))

borough_avg <- train_set %>%
  left_join(code_avg, by="lsoa_code") %>%
  group_by(borough) %>%
  summarize(b_b = mean(value - mu - b_c))

major_avg <- train_set %>%
  left_join(code_avg, by="lsoa_code") %>%
  left_join(borough_avg, by="borough")%>%
  group_by(major_category) %>%
  summarize(b_ma = mean(value - mu - b_c - b_b))

minor_avg <- train_set %>%
  left_join(code_avg, by="lsoa_code") %>%
  left_join(borough_avg, by="borough")%>%
  left_join(major_avg, by="major_category")%>%
  group_by(minor_category)%>%
  summarize(b_mi = mean(value - mu - b_c - b_b - b_ma))

year_avg <- train_set %>%
  left_join(code_avg, by="lsoa_code") %>%
  left_join(borough_avg, by="borough")%>%
  left_join(major_avg, by="major_category")%>%
  left_join(minor_avg, by="minor_category")%>%
  group_by(year)%>%
  summarize(b_y = mean(value - mu - b_c - b_b - b_ma - b_mi))

month_avg <- train_set %>%
  left_join(code_avg, by="lsoa_code") %>%
  left_join(borough_avg, by="borough")%>%
  left_join(major_avg, by="major_category")%>%
  left_join(minor_avg, by="minor_category")%>%
  left_join(year_avg, by="year")%>%
  group_by(month)%>%
  summarize(b_m = mean(value - mu - b_c - b_b - b_ma - b_mi - b_y))

predicted_values <- test_set %>% 
  left_join(code_avg, by="lsoa_code") %>%
  left_join(borough_avg, by="borough")%>%
  left_join(major_avg, by="major_category")%>%
  left_join(minor_avg, by="minor_category")%>%
  left_join(year_avg, by="year")%>%
  left_join(month_avg, by="month")%>%
  mutate(pred = mu + b_c + b_b + b_ma + b_mi + b_y +b_m) %>%
  pull(pred)

#now here are the effects accounted for
all_effects <- RMSE(predicted_values, test_set$value)
all_effects

```

As noted previously however there are ways to possibly improve the predicted values. For instance values in the dataset are whole integers. As there are numerous records close to either zero, one, or two this report feels it safe to possibly round predicted values downward. Another possibility are the occurrences of negative values is impossible as there cannot be negative crimes. Therein this report can safely assume all negative values can be counted as zero values. Taking into these possibilities into account here are the predicted values with these assumptions.

```{r, warning = FALSE}

#now the effects rounded downward
all_effects_rounded <- RMSE(floor(predicted_values), test_set$value)
all_effects_rounded


#effects but all negative numbers rounded to zero
predicted_values_no_negative <- ifelse(predicted_values <= 0, 0, predicted_values)

all_effects_added_no_negatives <- RMSE(predicted_values_no_negative, test_set$value)
all_effects_added_no_negatives

rmse_results <- rmse_results %>% add_row(method = "All Effects", 
                                         RMSE = all_effects_added_no_negatives)
```

As seen above by taking the negative values and assuming they were in fact zero has improved the accuracy of the predicted values. Moving onward regularization needs to be taken into account.

## 3.3 Regularizing the Dataset
Regularization takes into account the fact that some places simply have more records or instances of reports than other. Seen in a model below based on Professor Irizarry's model in his Data Science course, with the idea here is to use a tuning parameter to account for these differences (Irizarry, 2021, Penalized least squares).

$$
\hat{v}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$
Now shown below are many possible tuning parameters to use with a plot to demonstrate the tuning parameters. Of note is that this report also takes into account the need to set negative values to zero before applying the prediction. Note that for the sake of brevity only the code utilizing the best tuning parameter will be shown below.

```{r, warning = FALSE, message=FALSE, echo=FALSE}
set.seed(2021, sample.kind="Rounding")

#now utilize regularization for effects
lambdas <- seq(0, 25, 0.5)


set.seed(2021, sample.kind="Rounding")
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$value)
  
  b_c <- train_set %>% 
    group_by(lsoa_code) %>%
    summarize(b_c = sum(value - mu)/(n()+l))
  
  b_b <- train_set %>% 
    left_join(b_c, by="lsoa_code") %>%
    group_by(borough) %>%
    summarize(b_b = sum(value - b_c - mu)/(n()+l))
  
  b_ma <- train_set %>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    group_by(major_category) %>%
    summarize(b_ma = sum(value - b_c - b_b - mu)/(n()+l))
  
  b_mi <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    group_by(minor_category) %>%
    summarize(b_mi = sum(value - b_c - b_b - b_ma - mu)/(n()+l))
  
  b_y <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    group_by(year) %>%
    summarize(b_y = sum(value - b_c - b_b - b_ma - b_mi - mu)/(n()+l))
  
  b_m <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    left_join(b_y, by='year') %>%
    group_by(month) %>%
    summarize(b_m = sum(value - b_c - b_b - b_ma - b_mi - b_y - mu)/(n()+l))
  
  predicted_values <- 
    test_set %>% 
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    left_join(b_y, by='year') %>%
    left_join(b_m, by='month') %>%
    mutate(pred = mu + b_c + b_b + b_ma + b_mi + b_y + b_m) %>%
    pull(pred)
  
  predicted_values <- ifelse(predicted_values <= 0, 0, predicted_values)
  
  return(RMSE(predicted_values, test_set$value))
})
qplot(lambdas, rmses) 

```

With this done the best tuning parameter can be applied to the to a single version of the model shown above. As mentioned earlier here is the method utilizing the best lambda found previously with the model. the only difference being that there is no application of multiple tuning parameters to see which one works best.

```{r, warning = FALSE, message=FALSE}

#obtain best lambda
best_lambda <- lambdas[which.min(rmses)]

#now apply to the model above
set.seed(2021, sample.kind="Rounding")
mu <- mean(train_set$value)
  
b_c <- train_set %>% 
    group_by(lsoa_code) %>%
    summarize(b_c = sum(value - mu)/(n()+best_lambda))
  
b_b <- train_set %>% 
    left_join(b_c, by="lsoa_code") %>%
    group_by(borough) %>%
    summarize(b_b = sum(value - b_c - mu)/(n()+best_lambda))
  
b_ma <- train_set %>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    group_by(major_category) %>%
    summarize(b_ma = sum(value - b_c - b_b - mu)/(n()+best_lambda))
  
b_mi <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    group_by(minor_category) %>%
    summarize(b_mi = sum(value - b_c - b_b - b_ma - mu)/(n()+best_lambda))
  
b_y <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    group_by(year) %>%
    summarize(b_y = sum(value - b_c - b_b - b_ma - b_mi - mu)/(n()+best_lambda))
  
b_m <- train_set%>%
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    left_join(b_y, by='year') %>%
    group_by(month) %>%
    summarize(b_m = sum(value - b_c - b_b - b_ma - b_mi - b_y - mu)/(n()+best_lambda))
  
predicted_values <- 
    test_set %>% 
    left_join(b_c, by="lsoa_code")%>%
    left_join(b_b, by="borough") %>%
    left_join(b_ma, by="major_category") %>%
    left_join(b_mi, by='minor_category') %>%
    left_join(b_y, by='year') %>%
    left_join(b_m, by='month') %>%
    mutate(pred = mu + b_c + b_b + b_ma + b_mi + b_y + b_m) %>%
    pull(pred)
```

```{r, warning = FALSE}

#now set predicted values for negative to be zero and predict
predicted_values <- ifelse(predicted_values <= 0, 0, predicted_values)
values_regularized_effects <- RMSE(predicted_values, test_set$value)

rmse_results <- rmse_results %>% add_row(method = "Regularized All Effects", 
                                         RMSE = values_regularized_effects)

rmse_results %>% knitr::kable()


```

## 3.4 Reccomendation Model Results
At the start of the recommendation model the RMSE was value was rather poor but did improve when taking into account the effects and regularizing those effects. With that said the goal of an accuracy of minimally 70% was not met. Ultimately the recommendation model developed cannot be recommended, being ill-suited towards this type of analysis in its current form. The reason for this is likely due to the prevalence of values 0, 1, and 2. Now taking into account the possibility of analyzing the data in a categorical viewpoint as discussed above there may be an alternate way to create the model. For this however this report will go towards a model more proven to working with multitudes of categorical variables.

# 4.0 Categorically-Oriented Methods

## 4.1 Training Setup
Before beginning to test the various categorical methods with this dataset there remains one possible issue. Due to hardware limitations as well as speed/performance issues time to process in training can be strenuous. To solve this a sample of the training and testing sets, 10%, will be utilized to help the performance. With this done the report may now move onto other methods utilized.

```{r, warning = FALSE, message=FALSE}
#for training purposes only
#sample for faster usage
train_set_sample <- sample_frac(train_set, 0.1)
test_set_sample <- sample_frac(test_set, 0.1)

test_set_sample <- test_set_sample %>% 
  semi_join(train_set_sample, by = "borough")%>%
  semi_join(train_set_sample, by = "major_category")%>%
  semi_join(train_set_sample, by = "lsoa_code")%>%
  semi_join(train_set_sample, by = "minor_category")%>%
  semi_join(train_set_sample, by = "year")%>%
  semi_join(train_set_sample, by = "month")%>%
  semi_join(train_set_sample, by = "value")%>%
  semi_join(train_set_sample, by = "crime_occurred")


```

## 4.1 Nearest Neighbor Method
One method of analyzing the data in another method, though not one predicated on categorical variables, is to use kNN, or k-nearest neighbors, which involves using the observations in the data to make estimates via the nearest observation points as talked about on  Professor Irizarry's online githhub (Irizarry, 2021, Motivation with k-nearest neighbors). In implementing kNN this report will be using the crime_occurred column created earlier and leaving out the values column. Besides this in order to increase performance Also included is a plot to demonstrate how altering the k tuning value may improve the prediction outcomes.

```{r, warning = FALSE, message=FALSE}

#used knn via cv to increase performance

set.seed(2021, sample.kind="Rounding")
#now edit cross validation for performance, 

#due to computation time, this wll be used within the markdown
train_knn_cv <- train(crime_occurred ~ lsoa_code + borough + major_category + minor_category + year + month,
                      method = "knn",
                      data = train_set_sample,
                      tuneGrid = data.frame(k = seq(1, 7, 2)),
                      trControl = trainControl(method = "cv", number = 10, p = .9))

#see best fit as number and within plot
ggplot(train_knn_cv, highlight = TRUE)
train_knn_cv$bestTune

#prediction
knn_cv_preds <- predict(train_knn_cv, test_set_sample[c(1:4,6:7)])
knn_prediction <- mean(knn_cv_preds == test_set_sample$crime_occurred)

#copy prediction to table for future comparisons
prediction_results <- tibble(method = "kNN Method", Prediction = knn_prediction)

#see results
prediction_results %>% knitr::kable()
```

Looking at the above results this report still finds that it can do better as kNN, while usable with categorical data, may not be the best to use. Another possible method is shown below.

## 4.2 Rpart Regression Tree
Another method of analyzing the data and predicting results would be to use rpart, recursive partitioning. Here data is partitioned to create a tree of possible outcomes recursively, meaning that the current partition is utilized in determining future partitions and so on (Irizarry, 2021, Regression Trees). Again utilizing the crime_occurred column and not the values column and accounting for the effects of the other variables is shown below.

```{r, warning = FALSE, message=FALSE}

#rpart rounding 

#set.seed
set.seed(2021, sample.kind = "Rounding")    # simulate R 3.5

#train via rpart
train_rpart <- train(crime_occurred ~ lsoa_code + borough + major_category + minor_category + year + month, 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.2, 0.05)),
                     data = train_set_sample)

#see rpart tune and plot
train_rpart$bestTune
ggplot(train_rpart, highlight = TRUE)

#prediction
rpart_preds <- predict(train_rpart, test_set_sample[c(1:4,6:7)])
rpart_prediction <- mean(rpart_preds == test_set_sample$crime_occurred)

#add to table
prediction_results <- prediction_results %>% add_row(method = "RPart Method", 
                                                     Prediction = rpart_prediction)

#see results
prediction_results %>% knitr::kable()
```

## 4.3 Random Forest Method
The final independent method that this report will be looking at is the random forest method. This method, somewhat similar to rpart, involves the usage of decision trees but in addition to this the method averages decision trees together (Irizarry, 2021, Random Forest). Partly this is done via the size of node utilized in the random forest, but for this report in particular this will be left alone. Shown below is the code in creating the random forest prediction. In addition a look at the important variables in this random forest is shown as well.

```{r, warning = FALSE, message=FALSE}

#random forest

#set.seed
set.seed(2021, sample.kind = "Rounding")
train_rf <- train(crime_occurred ~ lsoa_code + borough + major_category + minor_category + year + month,
                  data = train_set_sample,
                  method = "rf",
                  ntree = 50,
                  nodesize=30,
                  tuneGrid = data.frame(mtry = seq(1:3)))
#see best tuning
train_rf$bestTune


#create prediction
rf_preds <- predict(train_rf, test_set_sample[c(1:4,6:7)])
randForest_prediction <- mean(rf_preds == test_set_sample$crime_occurred)

#add to table
prediction_results <- prediction_results %>% add_row(method = "Random Forest Method", 
                                                     Prediction = randForest_prediction)
#see important variables
varImp(train_rf)

#see results
prediction_results %>% knitr::kable()


```

## 4.4 Quick Comparison

Now that this is done all models can be compared to one another to see which model had the best predictive power. As shown below all models performed admirably. With that said the best model, the random forest method, while securing the highest predictive accuracy also took the longest to perform. Due to this and the coupled fact that the final model would be approximately 10x more the model utilized in the final test will be the rpart method due to its high predictive accurracy and its faster runtime.

```{r, warning = FALSE, message=FALSE, echo=FALSE}

prediction_results %>% knitr::kable()

#cleanup
rm(RMSE, rpart_preds, rf_preds, p, knn_cv_preds,
   train_set, train_set_sample, train_rpart, train_rf, train_knn_cv, train_knn,
   test_index, test_set, test_set_sample, randomForest_prediction,
   rpart_prediction, knn_prediction, degree,
   prediction_results, randForest_prediction)


```

# 5.0 London Crimes: Final Testing

## 5.1 London Crimes: Final Test Method 
With the above fine-tuning of the above methods and algorithms done, and the best predictive model that also performs quickly is the rpart method, this report can now apply it to the london and final_test datasets

## 5.2 End Results
To begin firstly this report must utilize rpart method to the london dataset. Once done this can then be compared to the final_test dataset. From there an accurracy mirroring what was found in the training set, perhaps a bit less due to working with more data, can be achieved. Below is the final model with the results.

```{r, warning = FALSE, echo=FALSE, message=FALSE}

#set.seed
set.seed(2021, sample.kind = "Rounding")    # simulate R 3.5

#train via rpart
rpart_final <- train(crime_occurred ~ lsoa_code + borough + major_category + minor_category + year + month, 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.2, 0.05)),
                     data = london)


#prediction
final_preds <- predict(rpart_final, final_test[c(1:4,6:7)])
final_prediction <- mean(final_preds == final_test$crime_occurred)

#add to table
prediction_results <- prediction_results %>% add_row(method = "Final Method: Rpart", 
                                                     Prediction = final_prediction)

#see results
prediction_results %>% knitr::kable()

```

# 6.0 Conclusion

## 6.1 Report Summary
This report went over the thought process of analyzing the results of london crimes from the year 2010 onward and find a predictive model that reached at minimum 70%. Discarding a more linear regressive recommendation model and the values, the count of monthly crime incidences, for a broader look at the data as a binary "yes" or "no" viewpoint was implemented within the report. After testing more categorical methods a final method was created via utilizing three sub-methods to increase accuracy. With this done the final method ultimately met the goal of predicting with an accuracy of at least 70%.

## 6.2 Improvement Postulation
Looking back on this report there is much room for improvement. To begin better fine tuning could have been utilized, for instance with kNN and its k parameter and for random forest and its ntree parameter were lowered for the sake of performance and speed. In fact speed was the deciding factor in utilizing the rpart method over the random forest method as the final model due to its faster completion time. Undoubtedly there could have been more tuning the recommendation view of the data, however for this report it felt prudent to have a method that did not work quite as well for a comparative.

Exploration, while satisfactory overall, could have been improved with better nuisances in visuals, specifically with more diverse plots. Besides this some explanatory information was cut for the sake of keeping this report within a respectable length. Other than that minor changes in the overall structure of this report, such as choosing whether to sample training data or not and what year to begin trimming the data, while not harmful did take an unexpected amount of time when compounded with one another.

Hardware limitations were also a grave handicap for this report. Often processes on their own simply took too long to reasonably utilize due to speed issues that better hardware could have solved. Due to this training was sampled for the sake of speed and the overall size of the data taken from the original source, london_crimes, was shrunk from approximately 13 million records to around 53,000 records. Most tragically the code to utilize an ensemble method ultimately remained unused due to the process time it'd take to utilize such a model. In general this report would have benefited greatly with better hardware, which as time moves on will become more available.

## 6.3 Final Thoughts
In general this report was an interesting exploration into independent research into various methods to analyze data. The handicaps and struggles faced throughout it were likely also faced by others in their own analyzes as well. With that said, as a first stepping stone into the field of Data Science, this report hopes to have been informative and educational in its usage. I, the author, would personally like to thank Professor Irizarry and his staff in their Data Science course. Also thank you for reading this report and hopefully looking forward to more as well.


\newpage

# References
* Cornell (2021). Criminal Law https://www.law.cornell.edu/wex/criminal_law. Accessed 7/01/2021.

* Boysen, J (2017). Kaggle: London Crime Data, 2008-2016. https://www.kaggle.com/jboysen/london-crime. Accessed 6/24/2021.

* Irizarry, R (2021). Introduction to Data Science: Loss function. https://rafalab.github.io/dsbook/large-datasets.html#netflix-loss-function. 

* Irizarry, R (2021). Introduction to Data Science: A first model. https://rafalab.github.io/dsbook/large-datasets.html#a-first-model.

* Irizarry, R (2021). Introduction to Data Science: Modeling movie effects. https://rafalab.github.io/dsbook/large-datasets.html#modeling-movie-effects.

* Irizarry, R (2021). Introduction to Data Science: Penalized least squares. https://rafalab.github.io/dsbook/large-datasets.html#penalized-least-squares.

* Irizarry, R (2021). Introduction to Data Science: Motivation with k-nearest neighbors. https://rafalab.github.io/dsbook/cross-validation.html#knn-cv-intro

* Irizarry, R (2021). Introduction to Data Science: Regression Trees. https://rafalab.github.io/dsbook/examples-of-algorithms.html#regression-trees

* Irizarry, R (2021). Introduction to Data Science: Random Forest. https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests

